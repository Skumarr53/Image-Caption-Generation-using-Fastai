
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/models_03.ipynb

import torch
import torchvision.models as models
from torch import nn
from pdb import set_trace

device = torch.device('cuda' if torch.cuda.is_available() else "cpu")

class Encoder(nn.Module):
    '''
    Encoder.

    '''

    def __init__(self, encoded_image_size=14, model = models.resnet101(pretrained=True)):
        super(Encoder, self).__init__()

        #encoded image size output of encoder and goes into decoder as input
        self.enc_image_size = encoded_image_size

        # what resnet model we want use as encoder by default 'resnet34' is used
        # but detached last two layers (AdaptiveAvgPool2d and Linear) as we are not
        # doing classification
        layers = list(model.children())[:-2]
        self.model = nn.Sequential(*layers)

        #Resize image
        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))

        self.fine_tune()

    def forward(self, images):
        """
        takes images batch in tensor of  dimesionas (batch_size, 3, image_size, image_size)
        """

        out = self.model(images)
        out = self.adaptive_pool(out)
        out = out.permute(0,2,3,1)
        return out

    def fine_tune(self, fine_tune=True):
        """
        Allows or prevent the computation of gradients for convolutional blocks

        """

        for p in self.model.parameters():
            p.requires_grad = False
        # if fine-tuning, only fine-tune convolution blocks 2 through 4
        for c in list(self.model.children())[5:]:
            for p in self.model.parameters():
                p.requires_grad = fine_tune


class Attention(nn.Module):
    """
    Attention Network

    """
    def __init__(self, encoder_dim, decoder_dim, attention_dim):
        
        '''

        :param encoder_dim: feature size of encoded images
        :param decoder_dim: size of decoder's RNN
        :param attention_dim: size of the attention network

        encoder_dim is feature_size equivalent of word_embedding in the context
        of machine translation and attention-dim is the no of encoded images equivalent
        to no of words to be tranlated

        on the decoder recieves different size (size it processes cations it
        has the  size of word_embeddings) unlikely in machine tranlation where
        feature size is word embedding size for both decoder and encoder.

        decoder_dim = word embedding vec dim
        encoder dim = pixels ecodeded images
        Inoreder to compare both we should reduce them to the common dim i.e attention_dim
        '''

        super(Attention, self).__init__()
        self.encoder_att = nn.Linear(encoder_dim, attention_dim)
        self.decoder_att = nn.Linear(decoder_dim, attention_dim)
        self.full_att = nn.Linear(attention_dim,1)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    '''
     This has to run for every step in decoding part it takes previous decoder
     hidden and computes attention weighted endcoder [att scores * encoder output]
     more attention given to word not predicted yet.
    '''
    def forward(self, encoder_out, decoder_hidden):
        att1 = self.encoder_att(encoder_out) # input in LSTM
        att2 = self.decoder_att(decoder_hidden) # prevoius hidden state hidden state representation
        #we have reduced them to attention dimension lets do [prevoius hidden state + input] current state
        att  = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) #(batch size, num_pixels)
        alpha = self.softmax(att) #attention scores (batch_size, num_pixels)
        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)
        return attention_weighted_encoding, alpha

class DecoderWithAttention(nn.Module):
    """
    Decoder

    """

    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout = 0.5):

        """
        :param attention_dim: size of attention network
        :param embed_dim: embedding size
        :param decoder_dim: size of decoder's RNN
        :param vocab_size: size of vocabulary
        :param encoder_dim: feature size of encoded images
        :param dropout: dropout
        """

        super(DecoderWithAttention, self).__init__()

        self.encoder_dim = encoder_dim
        self.attention_dim = attention_dim
        self.embed_dim = embed_dim
        self.decoder_dim = decoder_dim
        self.vocab_size = vocab_size
        self.dropout = dropout

        # this is modified [attention weighted] input goes into LSTM decoder
        self.attention = Attention(encoder_dim, decoder_dim, attention_dim) # attention network

        self.embedding = nn.Embedding(vocab_size, embed_dim) #word embbeding tensor
        self.dropout = nn.Dropout(p=self.dropout)

        # concatinating
        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)
        # may be we are concatinating input and embed

        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell
        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell
        self.f_beta = nn.Linear(decoder_dim, encoder_dim)
        self.sigmoid = nn.Sigmoid()
        self.fc = nn.Linear(decoder_dim, vocab_size) # linear layer to find scores over vocabulary
        self.init_weights() #

    def init_weights(self):
        """
        Initilizes some parametes with values from the uniform Dist

        """

        self.embedding.weight.data.uniform_(0.1, 0.1)
        self.fc.bias.data.fill_(0)
        self.fc.weight.data.uniform_(-0.1,0.1)

    def load_pretrained_embeddings(self, embeddings):
        """
        Loads embedding layer with pre-trained embeddings

        """
        # adding an embedding
        self.embedding.weight = nn.Parameter(embeddings)

    def fine_tune_embeddings(self, fine_tune=True):

        for p in self.embedding.parameters():
            p.requires_grad = fine_tune

    def init_hidden_state(self, encoder_out):
        """

        """
        mean_encoder_out = encoder_out.mean(dim=1)
        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)
        c = self.init_c(mean_encoder_out)
        return h, c

    def forward(self, encoder_out, encoded_captions, decode_lengths,inds):

        """
        Forward propagation.
        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)
        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)
        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)
        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices
        """

        batch_size = encoder_out.size(0)
        encoder_dim = encoder_out.size(-1)
        vocab_size = self.vocab_size

        # Flatten image
        encoder_out  = encoder_out.view(batch_size, -1, encoder_dim) # (batch_size, num_pixels, encoder_dim)
        num_pixels = encoder_out.size(1)

        # Sort input data by decreasing captions lenghts;
        #caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)
        #encoder_out = encoder_out[sort_ind]
        #encoded_captions = encoded_captions[sort_ind]

        # Embeddings
        embeddings = self.embedding(encoded_captions).type(torch.cuda.FloatTensor)
        # Initialize LSTM state
        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)

        # we won't decode at the <end> position, since we've finished
        # generationg as soon  generate as <end> so, decoding lengths are actual lengths-1
        decode_lengths = (decode_lengths -1).tolist()

        # create tensors to hold word precisions scores and alphas
        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)
        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)


        for t in range(max(decode_lengths)):
            batch_size_t = sum([l > t for l in decode_lengths])
            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],  ## I have flipped h part to match dimension
                                                                (h[:batch_size_t]))
            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))
            attention_weighted_encoding = (gate * attention_weighted_encoding)
            h, c = self.decode_step(torch.cat([embeddings[:batch_size_t,t,:],
                                             attention_weighted_encoding],dim=1)
                                    ,(h[:batch_size_t],c[:batch_size_t])) #(batch_size_t, decoder_dim)
            preds = self.fc(self.dropout(h))
            predictions[:batch_size_t, t, :] = preds
            alphas[:batch_size_t, t, :]  = alpha

        return predictions, decode_lengths, alphas, inds